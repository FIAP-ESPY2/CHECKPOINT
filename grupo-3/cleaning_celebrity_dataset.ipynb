{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook de Limpeza de Dataset de Celebridades Famosas\n",
        "\n",
        "Este notebook demonstra um processo completo de limpeza de dados para um dataset de celebridades, incluindo:\n",
        "\n",
        "- Tratamento de valores faltantes\n",
        "- Padroniza√ß√£o de dados\n",
        "- Remo√ß√£o de duplicatas\n",
        "- Valida√ß√£o de qualidade\n",
        "- Visualiza√ß√µes\n",
        "- Exporta√ß√£o do dataset limpo\n",
        "\n",
        "---"
      ]
    }\n  },\n  {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n      \"## üìã Relat√≥rio de Limpeza\"\n    ]\n  },\n  {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n      \"def generate_cleaning_report(df_original, df_clean, cleaning_log):\\n\",\n      \"    \\\"\\\"\\\"Gera relat√≥rio detalhado do processo de limpeza\\\"\\\"\\\"\\n\",\n      \"    \\n\",\n      \"    print(\\\"üìã RELAT√ìRIO DE LIMPEZA DE DADOS\\\")\\n\",\n      \"    print(\\\"=\\\" * 50)\\n\",\n      \"    \\n\",\n      \"    # Informa√ß√µes gerais\\n\",\n      \"    print(f\\\"\\\\nüïí Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\")\\n\",\n      \"    print(f\\\"üìä Dataset Original: {df_original.shape[0]} linhas, {df_original.shape[1]} colunas\\\")\\n\",\n      \"    print(f\\\"üìä Dataset Limpo: {df_clean.shape[0]} linhas, {df_clean.shape[1]} colunas\\\")\\n\",\n      \"    print(f\\\"üìâ Redu√ß√£o: {df_original.shape[0] - df_clean.shape[0]} linhas removidas\\\")\\n\",\n      \"    print(f\\\"üìà Novas colunas: {df_clean.shape[1] - df_original.shape[1]}\\\")\\n\",\n      \"    \\n\",\n      \"    # Log de opera√ß√µes\\n\",\n      \"    print(f\\\"\\\\nüîß OPERA√á√ïES REALIZADAS:\\\")\\n\",\n      \"    for i, operation in enumerate(cleaning_log, 1):\\n\",\n      \"        print(f\\\"  {i}. {operation}\\\")\\n\",\n      \"    \\n\",\n      \"    # Resumo das colunas\\n\",\n      \"    print(f\\\"\\\\nüìã RESUMO DAS COLUNAS:\\\")\\n\",\n      \"    for col in df_clean.columns:\\n\",\n      \"        dtype = df_clean[col].dtype\\n\",\n      \"        non_null = df_clean[col].notna().sum()\\n\",\n      \"        null_count = df_clean[col].isna().sum()\\n\",\n      \"        unique_vals = df_clean[col].nunique()\\n\",\n      \"        \\n\",\n      \"        print(f\\\"  üìå {col}:\\\")\\n\",\n      \"        print(f\\\"     - Tipo: {dtype}\\\")\\n\",\n      \"        print(f\\\"     - Valores n√£o-nulos: {non_null} ({(non_null/len(df_clean)*100):.1f}%)\\\")\\n\",\n      \"        print(f\\\"     - Valores nulos: {null_count} ({(null_count/len(df_clean)*100):.1f}%)\\\")\\n\",\n      \"        print(f\\\"     - Valores √∫nicos: {unique_vals}\\\")\\n\",\n      \"    \\n\",\n      \"    return {\\n\",\n      \"        'original_shape': df_original.shape,\\n\",\n      \"        'clean_shape': df_clean.shape,\\n\",\n      \"        'operations': cleaning_log,\\n\",\n      \"        'timestamp': datetime.now()\\n\",\n      \"    }\\n\",\n      \"\\n\",\n      \"# Gerar relat√≥rio\\n\",\n      \"report = generate_cleaning_report(df_raw, df_clean, log)\"\n    ]\n  },\n  {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n      \"## üíæ Exporta√ß√£o do Dataset Limpo\"\n    ]\n  },\n  {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n      \"def export_clean_dataset(df, filename_prefix=\\\"celebrities_clean\\\"):\\n\",\n      \"    \\\"\\\"\\\"Exporta o dataset limpo em m√∫ltiplos formatos\\\"\\\"\\\"\\n\",\n      \"    \\n\",\n      \"    timestamp = datetime.now().strftime(\\\"%Y%m%d_%H%M%S\\\")\\n\",\n      \"    exported_files = {}\\n\",\n      \"    \\n\",\n      \"    # CSV\\n\",\n      \"    csv_filename = f\\\"{filename_prefix}_{timestamp}.csv\\\"\\n\",\n      \"    df.to_csv(csv_filename, index=False, encoding='utf-8')\\n\",\n      \"    exported_files['csv'] = csv_filename\\n\",\n      \"    print(f\\\"‚úÖ Exportado para CSV: {csv_filename}\\\")\\n\",\n      \"    \\n\",\n      \"    # Excel\\n\",\n      \"    try:\\n\",\n      \"        excel_filename = f\\\"{filename_prefix}_{timestamp}.xlsx\\\"\\n\",\n      \"        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\\n\",\n      \"            df.to_excel(writer, sheet_name='Celebrities_Clean', index=False)\\n\",\n      \"            \\n\",\n      \"            # Adicionar metadados\\n\",\n      \"            metadata = pd.DataFrame({\\n\",\n      \"                'M√©trica': ['Total de registros', 'Total de colunas', 'Data de limpeza'],\\n\",\n      \"                'Valor': [len(df), len(df.columns), datetime.now().strftime('%Y-%m-%d %H:%M:%S')]\\n\",\n      \"            })\\n\",\n      \"            metadata.to_excel(writer, sheet_name='Metadata', index=False)\\n\",\n      \"        \\n\",\n      \"        exported_files['excel'] = excel_filename\\n\",\n      \"        print(f\\\"‚úÖ Exportado para Excel: {excel_filename}\\\")\\n\",\n      \"    except ImportError:\\n\",\n      \"        print(\\\"‚ö†Ô∏è  Biblioteca openpyxl n√£o dispon√≠vel. Exporta√ß√£o Excel omitida.\\\")\\n\",\n      \"    \\n\",\n      \"    # JSON\\n\",\n      \"    json_filename = f\\\"{filename_prefix}_{timestamp}.json\\\"\\n\",\n      \"    df.to_json(json_filename, orient='records', indent=2, force_ascii=False)\\n\",\n      \"    exported_files['json'] = json_filename\\n\",\n      \"    print(f\\\"‚úÖ Exportado para JSON: {json_filename}\\\")\\n\",\n      \"    \\n\",\n      \"    return exported_files\\n\",\n      \"\\n\",\n      \"# Exportar dataset limpo\\n\",\n      \"exported_files = export_clean_dataset(df_clean)\"\n    ]\n  },\n  {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n      \"## üìä Dataset Final\"\n    ]\n  },\n  {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n      \"# Visualiza√ß√£o final do dataset limpo\\n\",\n      \"print(\\\"\\\\nüìä DATASET LIMPO FINAL:\\\")\\n\",\n      \"print(\\\"=\\\" * 60)\\n\",\n      \"\\n\",\n      \"print(\\\"\\\\nüìä INFORMA√á√ïES GERAIS:\\\")\\n\",\n      \"print(f\\\"Shape: {df_clean.shape}\\\")\\n\",\n      \"print(f\\\"Mem√≥ria: {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\\\")\\n\",\n      \"\\n\",\n      \"print(\\\"\\\\nüìä PRIMEIRAS 10 LINHAS:\\\")\\n\",\n      \"display(df_clean.head(10))\\n\",\n      \"\\n\",\n      \"print(\\\"\\\\nüìä ESTAT√çSTICAS DESCRITIVAS:\\\")\\n\",\n      \"display(df_clean.describe(include='all'))\\n\",\n      \"\\n\",\n      \"print(\\\"\\\\nüìä INFORMA√á√ïES DAS COLUNAS:\\\")\\n\",\n      \"print(df_clean.info())\\n\",\n      \"\\n\",\n      \"print(\\\"\\\\nüéâ PROCESSO DE LIMPEZA CONCLU√çDO COM SUCESSO!\\\")\\n\",\n      \"print(\\\"=\\\" * 60)\\n\",\n      \"print(f\\\"‚úÖ Dataset original: {len(df_raw)} registros\\\")\\n\",\n      \"print(f\\\"‚úÖ Dataset limpo: {len(df_clean)} registros\\\")\\n\",\n      \"print(f\\\"‚úÖ Taxa de reten√ß√£o: {(len(df_clean)/len(df_raw)*100):.1f}%\\\")\\n\",\n      \"print(f\\\"‚úÖ Arquivos exportados: {len(exported_files)}\\\")\\n\",\n      \"\\n\",\n      \"# Salvar dataset limpo como vari√°vel para uso posterior\\n\",\n      \"celebrities_clean = df_clean.copy()\\n\",\n      \"print(\\\"\\\\nüíæ Dataset limpo salvo na vari√°vel 'celebrities_clean' para uso posterior!\\\")\"\n    ]\n  },\n  {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n      \"## üîÑ Carregamento de Dataset Real (Opcional)\\n\",\n      \"\\n\",\n      \"Se voc√™ tiver um arquivo CSV real de celebridades, descomente e execute o c√≥digo abaixo:\"\n    ]\n  },\n  {\n    \"cell_type\": \"code\",\n    \"execution_count\": null,\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n      \"# DESCOMENTE AS LINHAS ABAIXO PARA USAR SEU PR√ìPRIO DATASET\\n\",\n      \"\\n\",\n      \"# # Carregando dataset real\\n\",\n      \"# try:\\n\",\n      \"#     # Substitua 'seu_arquivo.csv' pelo caminho do seu arquivo\\n\",\n      \"#     df_real = pd.read_csv('celebrities_dataset.csv')\\n\",\n      \"#     \\n\",\n      \"#     print(f\\\"Dataset carregado: {df_real.shape}\\\")\\n\",\n      \"#     print(\\\"\\\\nPrimeiras linhas:\\\")\\n\",\n      \"#     display(df_real.head())\\n\",\n      \"#     \\n\",\n      \"#     # Aplicar o mesmo processo de limpeza\\n\",\n      \"#     print(\\\"\\\\nIniciando limpeza do dataset real...\\\")\\n\",\n      \"#     \\n\",\n      \"#     # Identificar problemas\\n\",\n      \"#     problems_real = identify_data_problems(df_real)\\n\",\n      \"#     \\n\",\n      \"#     # Aplicar limpeza (adapte conforme necess√°rio)\\n\",\n      \"#     df_real_clean, log_real = clean_celebrity_dataset(df_real)\\n\",\n      \"#     df_real_clean, log_real = complete_cleaning_process(df_real_clean)\\n\",\n      \"#     \\n\",\n      \"#     # An√°lise final\\n\",\n      \"#     analyze_dataset(df_real_clean, \\\"Dataset Real Limpo\\\")\\n\",\n      \"#     \\n\",\n      \"#     # Exportar\\n\",\n      \"#     exported_real = export_clean_dataset(df_real_clean, \\\"real_celebrities_clean\\\")\\n\",\n      \"#     \\n\",\n      \"#     print(\\\"‚úÖ Limpeza do dataset real conclu√≠da!\\\")\\n\",\n      \"#     \\n\",\n      \"# except FileNotFoundError:\\n\",\n      \"#     print(\\\"‚ö†Ô∏è  Arquivo n√£o encontrado. Certifique-se de que o arquivo existe no diret√≥rio correto.\\\")\\n\",\n      \"# except Exception as e:\\n\",\n      \"#     print(f\\\"‚ùå Erro ao carregar dataset: {e}\\\")\"\n    ]\n  },\n  {\n    \"cell_type\": \"markdown\",\n    \"metadata\": {},\n    \"source\": [\n      \"## üìö Resumo e Pr√≥ximos Passos\\n\",\n      \"\\n\",\n      \"### ‚úÖ O que foi realizado neste notebook:\\n\",\n      \"\\n\",\n      \"1. **Cria√ß√£o de dataset simulado** com problemas t√≠picos\\n\",\n      \"2. **An√°lise explorat√≥ria inicial** para identificar problemas\\n\",\n      \"3. **Identifica√ß√£o sistem√°tica de problemas** nos dados\\n\",\n      \"4. **Processo completo de limpeza:**\\n\",\n      \"   - Limpeza e padroniza√ß√£o de nomes\\n\",\n      \"   - Remo√ß√£o de duplicatas\\n\",\n      \"   - Valida√ß√£o de anos de nascimento\\n\",\n      \"   - Padroniza√ß√£o de categorias\\n\",\n      \"   - Tratamento de valores num√©ricos inv√°lidos\\n\",\n      \"   - C√°lculo de idade atual\\n\",\n      \"5. **An√°lise p√≥s-limpeza** com visualiza√ß√µes\\n\",\n      \"6. **Valida√ß√£o da qualidade** dos dados\\n\",\n      \"7. **Relat√≥rio detalhado** do processo\\n\",\n      \"8. **Exporta√ß√£o em m√∫ltiplos formatos** (CSV, Excel, JSON)\\n\",\n      \"\\n\",\n      \"### üöÄ Pr√≥ximos passos sugeridos:\\n\",\n      \"\\n\",\n      \"1. **Aplicar em dados reais:** Use a se√ß√£o opcional para processar seu pr√≥prio dataset\\n\",\n      \"2. **An√°lise mais profunda:** Explore correla√ß√µes e padr√µes nos dados limpos\\n\",\n      \"3. **Machine Learning:** Use os dados limpos para modelos de predi√ß√£o\\n\",\n      \"4. **Dashboards:** Crie visualiza√ß√µes interativas com Plotly ou Streamlit\\n\",\n      \"5. **Automatiza√ß√£o:** Transforme este processo em um pipeline de dados\\n\",\n      \"\\n\",\n      \"### üìñ Recursos adicionais:\\n\",\n      \"\\n\",\n      \"- [Pandas Documentation](https://pandas.pydata.org/docs/)\\n\",\n      \"- [Data Cleaning Best Practices](https://towardsdatascience.com/data-cleaning-with-python-using-pandas-library-c6f4a68ea8eb)\\n\",\n      \"- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)\\n\",\n      \"- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)\\n\",\n      \"\\n\",\n      \"---\\n\",\n      \"\\n\",\n      \"**üìß Para d√∫vidas ou sugest√µes, abra uma issue no reposit√≥rio!**\"\n    ]\n  }\n],\n\"metadata\": {\n  \"kernelspec\": {\n    \"display_name\": \"Python 3 (ipykernel)\",\n    \"language\": \"python\",\n    \"name\": \"python3\"\n  },\n  \"language_info\": {\n    \"codemirror_mode\": {\n      \"name\": \"ipython\",\n      \"version\": 3\n    },\n    \"file_extension\": \".py\",\n    \"mimetype\": \"text/x-python\",\n    \"name\": \"python\",\n    \"nbconvert_exporter\": \"python\",\n    \"pygments_lexer\": \"ipython3\",\n    \"version\": \"3.9.7\"\n  }\n},\n\"nbformat\": 4,\n\"nbformat_minor\": 4,
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Importa√ß√£o de Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importando as bibliotecas necess√°rias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, date\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Configura√ß√µes\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"NOTEBOOK DE LIMPEZA DE DATASET DE CELEBRIDADES\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé≠ Cria√ß√£o do Dataset de Exemplo\n",
        "\n",
        "Para demonstra√ß√£o, vamos criar um dataset simulado com problemas t√≠picos encontrados em dados reais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sample_dataset():\n",
        "    \"\"\"Cria um dataset simulado de celebridades para demonstra√ß√£o\"\"\"\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Dados simulados com problemas t√≠picos\n",
        "    names = [\n",
        "        'Leonardo DiCaprio', 'MERYL STREEP', 'leonardo dicaprio', 'Tom Hanks',\n",
        "        'Jennifer Lawrence', 'Will Smith', 'Scarlett Johansson', 'Robert Downey Jr.',\n",
        "        'Emma Stone', 'Brad Pitt', 'Angelina Jolie', 'Johnny Depp',\n",
        "        'Natalie Portman', 'Ryan Gosling', 'Emma Watson', 'Chris Evans',\n",
        "        'anne hathaway', 'Matt Damon', 'Sandra Bullock', 'George Clooney',\n",
        "        '', 'Tom Hanks', 'Jennifer Lawrence', None, 'UNKNOWN'\n",
        "    ]\n",
        "    \n",
        "    birth_years = [\n",
        "        1974, 1949, 1974, 1956, 1990, 1968, 1984, 1965,\n",
        "        1988, 1963, 1975, 1963, 1981, 1980, 1990, 1981,\n",
        "        1982, 1970, 1964, 1961, None, 1956, 1990, 2050, -100\n",
        "    ]\n",
        "    \n",
        "    professions = [\n",
        "        'Actor', 'actress', 'Actor', 'Actor', 'Actress', 'Actor',\n",
        "        'Actress', 'actor', 'Actress', 'Actor', 'actress', 'Actor',\n",
        "        'Actress', 'Actor', 'Actress', 'actor', 'Actress', 'Actor',\n",
        "        'Actress', 'Actor', '', 'Actor', 'Actress', None, 'Singer'\n",
        "    ]\n",
        "    \n",
        "    movies_count = [\n",
        "        25, 35, 25, 45, 12, 30, 20, 15, 18, 40, 25, 50,\n",
        "        22, 28, 15, 12, 20, 38, 28, 35, None, 45, 12, -5, 1000\n",
        "    ]\n",
        "    \n",
        "    awards = [\n",
        "        3, 8, 3, 5, 1, 0, 2, 1, 1, 2, 1, 1,\n",
        "        1, 2, 0, 0, 1, 3, 1, 4, None, 5, 1, 100, -2\n",
        "    ]\n",
        "    \n",
        "    net_worth_millions = [\n",
        "        260, 160, 260, 400, 130, 350, 165, 300, 30, 300, 120, 150,\n",
        "        90, 70, 85, 80, 35, 170, 250, 500, None, 400, 130, -50, 999999\n",
        "    ]\n",
        "    \n",
        "    # Criando o DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'name': names,\n",
        "        'birth_year': birth_years,\n",
        "        'profession': professions,\n",
        "        'movies_count': movies_count,\n",
        "        'awards_won': awards,\n",
        "        'net_worth_millions': net_worth_millions\n",
        "    })\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Criando o dataset de exemplo\n",
        "df_raw = create_sample_dataset()\n",
        "print(\"\\n1. DATASET ORIGINAL CRIADO\")\n",
        "print(f\"Shape: {df_raw.shape}\")\n",
        "print(\"\\nPrimeiras 10 linhas:\")\n",
        "print(df_raw.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç An√°lise Explorat√≥ria Inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_dataset(df, title=\"Dataset\"):\n",
        "    \"\"\"Realiza an√°lise explorat√≥ria b√°sica do dataset\"\"\"\n",
        "    \n",
        "    print(f\"\\nüìä AN√ÅLISE: {title}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Mem√≥ria usada: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
        "    \n",
        "    print(\"\\nüìã Informa√ß√µes das colunas:\")\n",
        "    print(df.info())\n",
        "    \n",
        "    print(\"\\nüîç Valores √∫nicos por coluna:\")\n",
        "    for col in df.columns:\n",
        "        unique_count = df[col].nunique()\n",
        "        print(f\"  {col}: {unique_count} valores √∫nicos\")\n",
        "    \n",
        "    print(\"\\n‚ùå Valores faltantes:\")\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing,\n",
        "        'Missing %': missing_pct\n",
        "    })\n",
        "    print(missing_df[missing_df['Missing Count'] > 0])\n",
        "    \n",
        "    print(\"\\nüìà Estat√≠sticas descritivas (colunas num√©ricas):\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(df[numeric_cols].describe())\n",
        "    \n",
        "    return missing_df\n",
        "\n",
        "# An√°lise inicial\n",
        "missing_analysis = analyze_dataset(df_raw, \"Dataset Original\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üö® Identifica√ß√£o de Problemas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def identify_data_problems(df):\n",
        "    \"\"\"Identifica problemas comuns no dataset\"\"\"\n",
        "    \n",
        "    problems = []\n",
        "    \n",
        "    # 1. Verificar duplicatas\n",
        "    duplicates = df.duplicated().sum()\n",
        "    if duplicates > 0:\n",
        "        problems.append(f\"üîÑ {duplicates} linhas duplicadas encontradas\")\n",
        "    \n",
        "    # 2. Verificar valores faltantes\n",
        "    missing_cols = df.columns[df.isnull().any()].tolist()\n",
        "    if missing_cols:\n",
        "        problems.append(f\"‚ùå Valores faltantes em: {missing_cols}\")\n",
        "    \n",
        "    # 3. Verificar inconsist√™ncias em nomes\n",
        "    if 'name' in df.columns:\n",
        "        name_issues = []\n",
        "        names = df['name'].dropna()\n",
        "        \n",
        "        # Nomes vazios ou apenas espa√ßos\n",
        "        empty_names = df['name'].str.strip().eq('').sum()\n",
        "        if empty_names > 0:\n",
        "            name_issues.append(f\"{empty_names} nomes vazios\")\n",
        "        \n",
        "        # Diferentes casos (mai√∫scula/min√∫scula)\n",
        "        unique_names = names.nunique()\n",
        "        unique_names_normalized = names.str.lower().str.strip().nunique()\n",
        "        if unique_names != unique_names_normalized:\n",
        "            name_issues.append(\"Inconsist√™ncias de capitaliza√ß√£o\")\n",
        "        \n",
        "        if name_issues:\n",
        "            problems.append(f\"üìù Problemas em nomes: {', '.join(name_issues)}\")\n",
        "    \n",
        "    # 4. Verificar valores extremos\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        data = df[col].dropna()\n",
        "        if len(data) > 0:\n",
        "            if col == 'birth_year':\n",
        "                current_year = datetime.now().year\n",
        "                invalid_years = ((data < 1800) | (data > current_year)).sum()\n",
        "                if invalid_years > 0:\n",
        "                    problems.append(f\"üìÖ {invalid_years} anos de nascimento inv√°lidos em '{col}'\")\n",
        "            \n",
        "            elif any(data < 0):\n",
        "                negative_count = (data < 0).sum()\n",
        "                problems.append(f\"‚ûñ {negative_count} valores negativos em '{col}'\")\n",
        "            \n",
        "            # Valores extremamente altos (outliers √≥bvios)\n",
        "            q99 = data.quantile(0.99)\n",
        "            extreme_high = (data > q99 * 10).sum()  # 10x o percentil 99\n",
        "            if extreme_high > 0:\n",
        "                problems.append(f\"üìà {extreme_high} valores extremamente altos em '{col}'\")\n",
        "    \n",
        "    print(\"üîç PROBLEMAS IDENTIFICADOS:\")\n",
        "    if problems:\n",
        "        for i, problem in enumerate(problems, 1):\n",
        "            print(f\"  {i}. {problem}\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Nenhum problema √≥bvio encontrado!\")\n",
        "    \n",
        "    return problems\n",
        "\n",
        "problems = identify_data_problems(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßπ Processo de Limpeza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_celebrity_dataset(df):\n",
        "    \"\"\"Realiza limpeza completa do dataset de celebridades\"\"\"\n",
        "    \n",
        "    df_clean = df.copy()\n",
        "    cleaning_log = []\n",
        "    \n",
        "    print(\"üßπ Iniciando processo de limpeza...\")\n",
        "    \n",
        "    # ETAPA 1: Limpeza de nomes\n",
        "    print(\"\\nüìù ETAPA 1: Limpeza de nomes\")\n",
        "    if 'name' in df_clean.columns:\n",
        "        initial_count = len(df_clean)\n",
        "        \n",
        "        # Remover linhas com nomes nulos ou vazios\n",
        "        df_clean['name'] = df_clean['name'].astype(str)\n",
        "        mask = (df_clean['name'].str.strip() != '') & \\\n",
        "               (df_clean['name'].str.lower() != 'nan') & \\\n",
        "               (df_clean['name'].str.upper() != 'UNKNOWN')\n",
        "        \n",
        "        removed_empty = initial_count - mask.sum()\n",
        "        df_clean = df_clean[mask].reset_index(drop=True)\n",
        "        \n",
        "        if removed_empty > 0:\n",
        "            cleaning_log.append(f\"Removidas {removed_empty} linhas com nomes inv√°lidos\")\n",
        "            print(f\"  ‚ùå Removidas {removed_empty} linhas com nomes inv√°lidos\")\n",
        "        \n",
        "        # Padronizar capitaliza√ß√£o (Title Case)\n",
        "        df_clean['name'] = df_clean['name'].str.title().str.strip()\n",
        "        cleaning_log.append(\"Padronizada capitaliza√ß√£o dos nomes\")\n",
        "        print(\"  ‚úÖ Capitaliza√ß√£o padronizada\")\n",
        "        \n",
        "        # Remover espa√ßos extras\n",
        "        df_clean['name'] = df_clean['name'].str.replace(r'\\s+', ' ', regex=True)\n",
        "        cleaning_log.append(\"Removidos espa√ßos extras dos nomes\")\n",
        "        print(\"  ‚úÖ Espa√ßos extras removidos\")\n",
        "    \n",
        "    return df_clean, cleaning_log\n",
        "\n",
        "# Executar primeira etapa da limpeza\n",
        "df_temp, log_temp = clean_celebrity_dataset(df_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_cleaning_process(df):\n",
        "    \"\"\"Completa o processo de limpeza\"\"\"\n",
        "    \n",
        "    df_clean = df.copy()\n",
        "    cleaning_log = []\n",
        "    \n",
        "    # ETAPA 2: Remover duplicatas\n",
        "    print(\"\\nüîÑ ETAPA 2: Remo√ß√£o de duplicatas\")\n",
        "    initial_count = len(df_clean)\n",
        "    df_clean = df_clean.drop_duplicates(subset=['name']).reset_index(drop=True)\n",
        "    duplicates_removed = initial_count - len(df_clean)\n",
        "    \n",
        "    if duplicates_removed > 0:\n",
        "        cleaning_log.append(f\"Removidas {duplicates_removed} linhas duplicadas\")\n",
        "        print(f\"  ‚ùå Removidas {duplicates_removed} linhas duplicadas\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Nenhuma duplicata encontrada\")\n",
        "    \n",
        "    # ETAPA 3: Limpeza de anos de nascimento\n",
        "    print(\"\\nüìÖ ETAPA 3: Limpeza de anos de nascimento\")\n",
        "    if 'birth_year' in df_clean.columns:\n",
        "        current_year = datetime.now().year\n",
        "        initial_invalid = len(df_clean[\n",
        "            (df_clean['birth_year'] < 1800) | \n",
        "            (df_clean['birth_year'] > current_year)\n",
        "        ])\n",
        "        \n",
        "        # Remover anos inv√°lidos (definir como NaN)\n",
        "        mask = (df_clean['birth_year'] >= 1800) & (df_clean['birth_year'] <= current_year)\n",
        "        df_clean.loc[~mask, 'birth_year'] = np.nan\n",
        "        \n",
        "        if initial_invalid > 0:\n",
        "            cleaning_log.append(f\"Corrigidos {initial_invalid} anos de nascimento inv√°lidos\")\n",
        "            print(f\"  ‚ö†Ô∏è  Corrigidos {initial_invalid} anos inv√°lidos (definidos como NaN)\")\n",
        "        else:\n",
        "            print(\"  ‚úÖ Todos os anos de nascimento s√£o v√°lidos\")\n",
        "    \n",
        "    # ETAPA 4: Padroniza√ß√£o de profiss√µes\n",
        "    print(\"\\nüé≠ ETAPA 4: Padroniza√ß√£o de profiss√µes\")\n",
        "    if 'profession' in df_clean.columns:\n",
        "        # Padronizar capitaliza√ß√£o\n",
        "        df_clean['profession'] = df_clean['profession'].str.title().str.strip()\n",
        "        \n",
        "        # Mapear profiss√µes similares\n",
        "        profession_mapping = {\n",
        "            'Actor': 'Actor',\n",
        "            'Actress': 'Actress', \n",
        "            '': np.nan,\n",
        "            'nan': np.nan\n",
        "        }\n",
        "        \n",
        "        df_clean['profession'] = df_clean['profession'].replace(profession_mapping)\n",
        "        cleaning_log.append(\"Profiss√µes padronizadas\")\n",
        "        print(\"  ‚úÖ Profiss√µes padronizadas\")\n",
        "    \n",
        "    # ETAPA 5: Limpeza de valores num√©ricos\n",
        "    print(\"\\nüî¢ ETAPA 5: Limpeza de valores num√©ricos\")\n",
        "    numeric_cols = ['movies_count', 'awards_won', 'net_worth_millions']\n",
        "    \n",
        "    for col in numeric_cols:\n",
        "        if col in df_clean.columns:\n",
        "            initial_invalid = len(df_clean[df_clean[col] < 0])\n",
        "            \n",
        "            # Remover valores negativos (definir como NaN)\n",
        "            df_clean.loc[df_clean[col] < 0, col] = np.nan\n",
        "            \n",
        "            # Remover outliers extremos\n",
        "            if col == 'movies_count':\n",
        "                df_clean.loc[df_clean[col] > 200, col] = np.nan\n",
        "            elif col == 'awards_won':\n",
        "                df_clean.loc[df_clean[col] > 50, col] = np.nan\n",
        "            elif col == 'net_worth_millions':\n",
        "                df_clean.loc[df_clean[col] > 50000, col] = np.nan\n",
        "    \n",
        "    # ETAPA 6: Calcular idade atual\n",
        "    print(\"\\nüéÇ ETAPA 6: C√°lculo de idade\")\n",
        "    if 'birth_year' in df_clean.columns:\n",
        "        current_year = datetime.now().year\n",
        "        df_clean['age'] = current_year - df_clean['birth_year']\n",
        "        cleaning_log.append(\"Coluna 'age' adicionada\")\n",
        "        print(\"  ‚úÖ Coluna 'age' calculada e adicionada\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Limpeza conclu√≠da!\")\n",
        "    print(f\"üìä Dataset final: {len(df_clean)} linhas, {len(df_clean.columns)} colunas\")\n",
        "    \n",
        "    return df_clean, cleaning_log\n",
        "\n",
        "# Completar limpeza\n",
        "df_clean, log = complete_cleaning_process(df_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä An√°lise do Dataset Limpo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compara√ß√£o antes/depois\n",
        "print(\"\\nüìä COMPARA√á√ÉO ANTES/DEPOIS:\")\n",
        "print(f\"Linhas: {len(df_raw)} ‚Üí {len(df_clean)} (diferen√ßa: {len(df_raw) - len(df_clean)})\")\n",
        "print(f\"Colunas: {len(df_raw.columns)} ‚Üí {len(df_clean.columns)} (diferen√ßa: {len(df_clean.columns) - len(df_raw.columns)})\")\n",
        "\n",
        "# An√°lise detalhada do dataset limpo\n",
        "analyze_dataset(df_clean, \"Dataset Limpo\")\n",
        "\n",
        "# Mostrar o dataset limpo\n",
        "print(\"\\nüìä DATASET LIMPO - PRIMEIRAS 10 LINHAS:\")\n",
        "display(df_clean.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualiza√ß√µes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar visualiza√ß√µes\n",
        "plt.style.use('default')\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('An√°lise do Dataset de Celebridades (Limpo)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Distribui√ß√£o de idades\n",
        "if 'age' in df_clean.columns:\n",
        "    ages = df_clean['age'].dropna()\n",
        "    axes[0,0].hist(ages, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0,0].set_title('Distribui√ß√£o de Idades')\n",
        "    axes[0,0].set_xlabel('Idade')\n",
        "    axes[0,0].set_ylabel('Frequ√™ncia')\n",
        "    axes[0,0].axvline(ages.mean(), color='red', linestyle='--', label=f'M√©dia: {ages.mean():.1f}')\n",
        "    axes[0,0].legend()\n",
        "\n",
        "# 2. Profiss√µes\n",
        "if 'profession' in df_clean.columns:\n",
        "    prof_counts = df_clean['profession'].value_counts()\n",
        "    axes[0,1].pie(prof_counts.values, labels=prof_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "    axes[0,1].set_title('Distribui√ß√£o por Profiss√£o')\n",
        "\n",
        "# 3. Rela√ß√£o entre filmes e pr√™mios\n",
        "if 'movies_count' in df_clean.columns and 'awards_won' in df_clean.columns:\n",
        "    data = df_clean[['movies_count', 'awards_won']].dropna()\n",
        "    axes[1,0].scatter(data['movies_count'], data['awards_won'], alpha=0.6, color='green')\n",
        "    axes[1,0].set_title('Filmes vs Pr√™mios')\n",
        "    axes[1,0].set_xlabel('N√∫mero de Filmes')\n",
        "    axes[1,0].set_ylabel('Pr√™mios Ganhos')\n",
        "\n",
        "# 4. Top 10 patrim√¥nios\n",
        "if 'net_worth_millions' in df_clean.columns:\n",
        "    top_wealth = df_clean.nlargest(10, 'net_worth_millions')[['name', 'net_worth_millions']]\n",
        "    axes[1,1].barh(top_wealth['name'], top_wealth['net_worth_millions'])\n",
        "    axes[1,1].set_title('Top 10 - Maior Patrim√¥nio')\n",
        "    axes[1,1].set_xlabel('Patrim√¥nio (Milh√µes USD)')\n",
        "    plt.setp(axes[1,1].get_yticklabels(), fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Valida√ß√£o da Qualidade dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_cleaned_data(df):\n",
        "    \"\"\"Valida a qualidade do dataset limpo\"\"\"\n",
        "    \n",
        "    validation_results = {}\n",
        "    \n",
        "    print(\"üîç EXECUTANDO VALIDA√á√ïES...\")\n",
        "    \n",
        "    # 1. Unicidade de nomes\n",
        "    if 'name' in df.columns:\n",
        "        unique_names = df['name'].nunique()\n",
        "        total_names = len(df)\n",
        "        validation_results['unique_names'] = unique_names == total_names\n",
        "        print(f\"  ‚úÖ Unicidade de nomes: {unique_names}/{total_names} √∫nicos\")\n",
        "    \n",
        "    # 2. Valores v√°lidos por coluna\n",
        "    validations = []\n",
        "    \n",
        "    if 'birth_year' in df.columns:\n",
        "        current_year = datetime.now().year\n",
        "        valid_years = df['birth_year'].between(1800, current_year, na_action='ignore').all()\n",
        "        validation_results['valid_birth_years'] = valid_years\n",
        "        validations.append(f\"Anos de nascimento: {'‚úÖ V√°lidos' if valid_years else '‚ùå Inv√°lidos'}\")\n",
        "    \n",
        "    for validation in validations:\n",
        "        print(f\"  {validation}\")\n",
        "    \n",
        "    # 3. Completude dos dados\n",
        "    print(\"\\nüìã COMPLETUDE DOS DADOS:\")\n",
        "
